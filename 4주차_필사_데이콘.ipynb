{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4주차_필사_데이콘.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMVxJiohywy6kCBPWNyJSjT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wbIdWkDy_qnQ"},"source":["#4. Word Embedding"]},{"cell_type":"markdown","metadata":{"id":"xtCEsnwHw8H5"},"source":["1.Keras Embedding Layer"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3JneiEBhv85t","executionInfo":{"status":"ok","timestamp":1632737822225,"user_tz":-540,"elapsed":93068,"user":{"displayName":"정세은","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17156544691030514202"}},"outputId":"48c20759-bbf8-4bee-db4b-5b9d161d81b9"},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"SnDpaZfnv68a","executionInfo":{"status":"ok","timestamp":1632737908038,"user_tz":-540,"elapsed":2344,"user":{"displayName":"정세은","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17156544691030514202"}}},"source":["import pandas as pd\n","path='/content/gdrive/MyDrive/Colab Notebooks/ESAA/1주차/train.csv'\n","train = pd.read_csv(path)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nt0RE0pDxNkO","executionInfo":{"status":"ok","timestamp":1632738030177,"user_tz":-540,"elapsed":6819,"user":{"displayName":"정세은","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17156544691030514202"}},"outputId":"e525a9f2-44ff-4be6-b40d-dbd8e341f5d9"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","def text2sequence(train_text, max_len=100):\n","    \n","    tokenizer = Tokenizer() #keras의 vectorizing 함수 호출\n","    tokenizer.fit_on_texts(train_text) #train 문장에 fit\n","    train_X_seq = tokenizer.texts_to_sequences(train_text) #각 토큰들에 정수 부여\n","    vocab_size = len(tokenizer.word_index) + 1 #모델에 알려줄 vocabulary의 크기 계산\n","    print('vocab_size : ', vocab_size)\n","    X_train = pad_sequences(train_X_seq, maxlen = max_len) #설정한 문장의 최대 길이만큼 padding\n","    \n","    return X_train, vocab_size, tokenizer\n","\n","train_X, vocab_size, vectorizer = text2sequence(train['text'], max_len = 100)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["vocab_size :  42331\n"]}]},{"cell_type":"code","metadata":{"id":"AHpvV3lk_4wE","executionInfo":{"status":"ok","timestamp":1632741859103,"user_tz":-540,"elapsed":321,"user":{"displayName":"정세은","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17156544691030514202"}}},"source":["max_len = 100\n","tokenizer = Tokenizer()\n","vocabulary = tokenizer.word_index"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"HoRpL1TcuS91","executionInfo":{"status":"ok","timestamp":1632738045449,"user_tz":-540,"elapsed":783,"user":{"displayName":"정세은","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17156544691030514202"}}},"source":["from keras.models import Sequential\n","from keras.layers import Dense, Activation,Embedding\n","model = Sequential()\n","model.add(Embedding(vocab_size, 128, input_length = 100))"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3nY2FJpSxZT6"},"source":["2.word2vec"]},{"cell_type":"code","metadata":{"id":"8TxZt69ruS7y","executionInfo":{"status":"ok","timestamp":1632741478207,"user_tz":-540,"elapsed":149726,"user":{"displayName":"정세은","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17156544691030514202"}}},"source":["tokenizer = Tokenizer()\n","\n","import gensim\n","word2vec = gensim.models.KeyedVectors.load_word2vec_format('/content/gdrive/MyDrive/Colab Notebooks/ESAA/4주차/GoogleNews-vectors-negative300.bin.gz', binary = True)"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"ybOoDb7DuS5r","executionInfo":{"status":"ok","timestamp":1632741867385,"user_tz":-540,"elapsed":279,"user":{"displayName":"정세은","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17156544691030514202"}}},"source":["embedding_matrix = np.zeros((vocab_size, 300)) #300차원의 임베딩 매트릭스 생성\n","\n","for index, word in enumerate(vocabulary): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.\n","    if word in word2vec: #넘겨 받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 뜻)\n","        embedding_vector = word2vec[word] #해당 토큰에 해당하는 vector를 불러오고\n","        embedding_mxtrix[i] = embedding_vector #해당 위치의 embedding_mxtrix에 저장합니다.\n","    else:\n","        print(\"word2vec에 없는 단어입니다.\")\n","        break"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"qap-tc4JuS3l","executionInfo":{"status":"ok","timestamp":1632741871681,"user_tz":-540,"elapsed":410,"user":{"displayName":"정세은","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17156544691030514202"}}},"source":["model = Sequential()\n","model.add(Embedding(vocab_size, 300,weights = [embedding_matrix], input_length = max_len))"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M6Y1wwPrxxjL"},"source":["3. glove"]},{"cell_type":"code","metadata":{"id":"sH0outSLuS0w","executionInfo":{"status":"ok","timestamp":1632742688105,"user_tz":-540,"elapsed":16994,"user":{"displayName":"정세은","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17156544691030514202"}}},"source":["import numpy as np\n","\n","# load the whole embedding into memory\n","glove = dict()\n","f = open('/content/gdrive/MyDrive/Colab Notebooks/ESAA/4주차/glove.txt')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    vector = np.asarray(values[1:], dtype='float32')\n","    glove[word] = vector\n","f.close()"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"pAdzYcLUuSxp","executionInfo":{"status":"ok","timestamp":1632742695917,"user_tz":-540,"elapsed":1377,"user":{"displayName":"정세은","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17156544691030514202"}}},"source":["embedding_matrix = np.zeros((vocab_size, 300)) #300차원의 임베딩 매트릭스 생성\n","\n","for index, word in enumerate(vocabulary): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.\n","    if word in glove: #넘겨 받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 뜻)\n","        embedding_vector = glove[word] #해당 토큰에 해당하는 vector를 불러오고\n","        embedding_mxtrix[i] = embedding_vector #해당 위치의 embedding_mxtrix에 저장합니다.\n","    else:\n","        print(\"glove 없는 단어입니다.\")\n","        break"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mgvh_ZpVuSux","executionInfo":{"status":"ok","timestamp":1632742917850,"user_tz":-540,"elapsed":310,"user":{"displayName":"정세은","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17156544691030514202"}}},"source":["model = Sequential()\n","model.add(Embedding(vocab_size, 300,weights = [embedding_matrix], input_length = max_len))"],"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C4PV_fZHy7hU"},"source":["4. Fasttext"]},{"cell_type":"code","metadata":{"id":"79iftj624gAp","executionInfo":{"status":"ok","timestamp":1632742999874,"user_tz":-540,"elapsed":10841,"user":{"displayName":"정세은","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17156544691030514202"}}},"source":["from gensim.models.keyedvectors import KeyedVectors\n","FastText = gensim.models.KeyedVectors.load_word2vec_format('/content/gdrive/MyDrive/Colab Notebooks/ESAA/4주차/fasttext.vec', binary=True, unicode_errors='ignore')"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"ypseZd63zHTh","executionInfo":{"status":"ok","timestamp":1632744027450,"user_tz":-540,"elapsed":662,"user":{"displayName":"정세은","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17156544691030514202"}}},"source":["embedding_matrix = np.zeros((vocab_size, 300)) #300차원의 임베딩 매트릭스 생성\n","\n","for index, word in enumerate(vocabulary): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.\n","    if word in word2vec: #넘겨 받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 뜻)\n","        embedding_vector = word2vec[word] #해당 토큰에 해당하는 vector를 불러오고\n","        embedding_mxtrix[i] = embedding_vector #해당 위치의 embedding_mxtrix에 저장합니다.\n"],"execution_count":48,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mwZDDNS80QeS"},"source":["5. Modeling"]},{"cell_type":"markdown","metadata":{"id":"z_JtL7Sy0vFs"},"source":["간단한 전처리 + 형태소 분석"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MYFDKy9bIdAG","executionInfo":{"status":"ok","timestamp":1632744109119,"user_tz":-540,"elapsed":7772,"user":{"displayName":"정세은","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17156544691030514202"}},"outputId":"bdf6762c-9610-4058-b9c4-c3d593287180"},"source":["!pip3 install konlpy"],"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n","\u001b[K     |████████████████████████████████| 19.4 MB 1.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n","Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n","Collecting colorama\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Collecting beautifulsoup4==4.6.0\n","  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 5.1 MB/s \n","\u001b[?25hCollecting JPype1>=0.7.0\n","  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 40.6 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n","Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n","Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n","  Attempting uninstall: beautifulsoup4\n","    Found existing installation: beautifulsoup4 4.6.3\n","    Uninstalling beautifulsoup4-4.6.3:\n","      Successfully uninstalled beautifulsoup4-4.6.3\n","Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6QCzzI6B0crV","executionInfo":{"status":"ok","timestamp":1632744510378,"user_tz":-540,"elapsed":401268,"user":{"displayName":"정세은","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17156544691030514202"}},"outputId":"9cd4c7a6-4127-43a9-fb02-e9a1ac3275c2"},"source":["from konlpy.tag import Okt\n","import re\n","import tqdm \n","\n","def text_preprocessing(text_list):\n","    \n","    stopwords = ['을', '를', '이', '가', '은', '는', 'null'] #불용어 설정\n","    tokenizer = Okt() #형태소 분석기 \n","    token_list = []\n","    \n","    for text in tqdm.tqdm(text_list):\n","        txt = re.sub('[^가-힣a-z]', ' ', text) #한글과 영어 소문자만 남기고 다른 글자 모두 제거\n","        token = tokenizer.morphs(txt) #형태소 분석\n","        token = [t for t in token if t not in stopwords or type(t) != float] #형태소 분석 결과 중 stopwords에 해당하지 않는 것만 추출\n","        token_list.append(token)\n","        \n","    return token_list, tokenizer\n","\n","#형태소 분석기를 따로 저장한 이유는 후에 test 데이터 전처리를 진행할 때 이용해야 되기 때문입니다. \n","train['token'], okt = text_preprocessing(train['text'])"],"execution_count":52,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 54879/54879 [06:36<00:00, 138.25it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"VjONhSTZ0sAz"},"source":["Vectorization"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w1PGDYrf0fp-","executionInfo":{"status":"ok","timestamp":1632744513920,"user_tz":-540,"elapsed":3597,"user":{"displayName":"정세은","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17156544691030514202"}},"outputId":"e4c497ff-8b9b-4d36-81c1-c4b626dd6812"},"source":["def text2sequence(train_text, max_len=1000):\n","    \n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(train_text)\n","    train_X_seq = tokenizer.texts_to_sequences(train_text)\n","    vocab_size = len(tokenizer.word_index) + 1\n","    print('vocab_size : ', vocab_size)\n","    X_train = pad_sequences(train_X_seq, maxlen = max_len)\n","    return X_train, vocab_size, tokenizer\n","\n","train_y = train['author']\n","train_X, vocab_size, vectorizer = text2sequence(train['token'], max_len = 100)\n","print(train_X.shape, train_y.shape)"],"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["vocab_size :  36342\n","(54879, 100) (54879,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"VmhWML4W0pSs"},"source":["Embedding"]},{"cell_type":"code","metadata":{"id":"Y6dva2310h5W","executionInfo":{"status":"ok","timestamp":1632744663434,"user_tz":-540,"elapsed":149520,"user":{"displayName":"정세은","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17156544691030514202"}}},"source":["word2vec = gensim.models.KeyedVectors.load_word2vec_format('/content/gdrive/MyDrive/Colab Notebooks/ESAA/4주차/GoogleNews-vectors-negative300.bin.gz', binary = True)\n","embedding_matrix = np.zeros((vocab_size, 300))\n","\n","for index, word in enumerate(vocabulary):\n","    if word in word2vec:\n","        embedding_vector = word2vec[word] \n","        embedding_mxtrix[i] = embedding_vector \n","    else:\n","        print(\"word2vec에 없는 단어입니다.\")\n","        break"],"execution_count":54,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uNoCWgof0k-8"},"source":["Modeling"]},{"cell_type":"code","metadata":{"id":"kxGKhDve0j6O","executionInfo":{"status":"ok","timestamp":1632744663435,"user_tz":-540,"elapsed":65,"user":{"displayName":"정세은","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17156544691030514202"}}},"source":["from keras.models import Sequential\n","from keras.layers import Dense, Activation, Embedding\n","\n","def LSTM(vocab_size, max_len=1000):\n","    model = Sequential()\n","    model.add(Embedding(vocab_size, 300,weights = [embedding_matrx], input_length = max_len)) #임베딩 가중치 적용 코드\n","    model.add(SpatialDropout1D(0.3))\n","    model.add(LSTM(64))\n","    model.add(Dropout(0.5))\n","    model.add(Dense(64, activation='relu', kernel_regularizer = regularizers.l2(0.001)))\n","    model.add(Dense(1, activation='sigmoid'))\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')\n","    model.summary()\n","    return model"],"execution_count":55,"outputs":[]}]}